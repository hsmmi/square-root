{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Project 3 on Mathematics in AI</center>\n",
    "\n",
    "Subject: Square Root\n",
    "\n",
    "Name: Hesam Mousavi\n",
    "\n",
    "Student number: 9931155\n",
    "\n",
    "<script type=\"text/x-mathjax-config\">\n",
    "MathJax.Hub.Config({\n",
    "tex2jax: {\n",
    "inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n",
    "processEscapes: true},\n",
    "jax: [\"input/TeX\",\"input/MathML\",\"input/AsciiMath\",\"output/CommonHTML\"],\n",
    "extensions: [\"tex2jax.js\",\"mml2jax.js\",\"asciimath2jax.js\",\"MathMenu.js\",\"MathZoom.js\",\"AssistiveMML.js\", \"[Contrib]/a11y/accessibility-menu.js\"],\n",
    "TeX: {\n",
    "extensions: [\"AMSmath.js\",\"AMSsymbols.js\",\"noErrors.js\",\"noUndefined.js\"],\n",
    "equationNumbers: {\n",
    "autoNumber: \"AMS\"\n",
    "}\n",
    "}\n",
    "});\n",
    "</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from my_io import create_random_walk_matrix, generate_dataset\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "matrix_size = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Idea\n",
    "\n",
    "### Using eigenvalue and eigenvector(eigendecomposition)\n",
    "\n",
    "Let's find the eigenvalue and eigenvector of $\\sqrt{A}$\n",
    "\n",
    "Just as with the real numbers, a real matrix may fail to have a real square root, but\n",
    "have a square root with **complex-valued** entries. Some matrices have **no square root**.\n",
    "\n",
    "### Positive semidefinite(P.S.D) matrices\n",
    "\n",
    "When matrix M is P.S.D? when M is **symmetric** matrix and\n",
    "$\\forall x \\rightarrow x^{\\top} M x \\geqslant 0$ then call M as P.S.D matrix\n",
    "\n",
    "A square real matrix is positive semidefinite if and only if $A=B^{\\top} B$ for some matrix B. There can be many different such matrices B. A positive semidefinite matrix A can also have many matrices B such that $A=B B$. However, A always has precisely one square root B(**principal**, **non-negative**, or **positive square root**) that is positive semidefinite (and hence symmetric).\n",
    "\n",
    "The principal square root of a real positive semidefinite matrix is real. The principal square root of a positive definite matrix is positive definite; more generally, the **rank** of the principal square root of A is the same as the rank of A.\n",
    "\n",
    "An nÃ—n matrix with n distinct nonzero eigenvalues has $2^{n}$ square roots. Such a matrix, A, has an **eigendecomposition** $V D^{1 / 2} V^{-1}$ where V is the matrix whose columns are eigenvectors of A and D is the diagonal matrix whose diagonal elements are the corresponding n eigenvalues $\\lambda_{i}$. Thus the **square roots** of A are given by $V D^{1 / 2} V^{-1}$, where $D^{1 / 2}$ is any square root matrix of D, which, for distinct eigenvalues, must be diagonal with diagonal elements equal to square roots of the diagonal elements of D; since there are two possible choices for a square root of each diagonal element of D, there are $2^{n}$ choices for the matrix $D^{1 / 2}$.(notes: 1- $D$ may have complex values. 2- $A$ must has $n$ linearly independent eigenvectors)\n",
    "\n",
    "So $\\sqrt{A} = A^{1 / 2} = V D^{1 / 2} V^{-1}$ because $\\left(V D^{\\frac{1}{2}} V^{-1}\\right)^{2}=V D^{\\frac{1}{2}}\\left(V^{-1} V\\right) D^{\\frac{1}{2}} V^{-1}=V D V^{-1}=A$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average error in 100 random matrices is 0.09996095845172331\n",
      "pause\n"
     ]
    }
   ],
   "source": [
    "error = 0\n",
    "\n",
    "for _ in range(100):\n",
    "    B = generate_dataset(matrix_size)\n",
    "    # B = create_random_walk_matrix(matrix_size)\n",
    "    # print(f'B:\\n{B}\\n')\n",
    "\n",
    "    # Eigendecomposition of a matrix\n",
    "    eigen_values, eigen_vectors = np.linalg.eig(B)\n",
    "    eigen_values = np.diag(eigen_values)\n",
    "\n",
    "    eigen_values_square = np.sqrt(eigen_values, dtype=np.complex_)\n",
    "    A = (eigen_vectors @ eigen_values_square @ np.linalg.pinv(eigen_vectors))\n",
    "    # A = A.astype(float)\n",
    "    reconstruct_B_with_rooted_square = np.round((A @ A), 3)\n",
    "    reconstruct_B_with_rooted_square = \\\n",
    "        reconstruct_B_with_rooted_square.astype(float)\n",
    "\n",
    "    # print(np.sum(A, axis=1))\n",
    "    # print(f'A @ A:\\n{reconstruct_B_with_rooted_square}\\n')\n",
    "\n",
    "    error += np.linalg.norm(\n",
    "        B - reconstruct_B_with_rooted_square, 'fro')\n",
    "\n",
    "print(f'The average error in 100 random matrices is {error/100}')\n",
    "print('pause')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> But it has some issues, what if the eigenvectors weren't independent? Then we don't have an inverse for it and we can't use pseudo inverse\n",
    "> \n",
    "> In some cases, even when it had inverse, the error was around 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Idea\n",
    "\n",
    "### Using alternate search method\n",
    "\n",
    "In each step I select a row of the matrix (vertex in the graph) and choose two elements of that row (two edges connected to our vertex) because it's a random walk, the sum of each row should be 1 so if I decrease one element by x, I must add other element x. to find which x should we use to decrease error(optimize our variable) I use two different methods:\n",
    "\n",
    "1. #### Adaptive learning rate\n",
    "\n",
    "2. #### Closed form\n",
    "\n",
    "### Adaptive learning rate(x)\n",
    "\n",
    "In this method simply consider the learning rate to be 1 at the first and then solve the problem until there is no improvement(after some specific iterations) then divide the learning rate by two until the learning rate leads to zero and by choosing deviation by two, all the best learning rate can be achievable(all numbers are the sum of 2-powers)\n",
    "\n",
    "### Closed form\n",
    "\n",
    "In this method, I write a quadratic equality equation based on error by changing x and find the best x by solving it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average error in 100 random matrices is by adaptive lerning rate is 0.0017269437784056738\n",
      "The average iteration in 100 random matrices by adaptive lerning rate is 25.8773\n",
      "The average error in 100 random matrices is by closed form rate is 0.0010748465881804984\n",
      "The average iteration in 100 random matrices by closed form rate is 41.7325\n",
      "pause\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from my_io import generate_dataset\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "matrix_size = 5\n",
    "\n",
    "decimal_point = 4\n",
    "\n",
    "random_test = 100\n",
    "\n",
    "\n",
    "def err_improve(B, A, i, j, k, x):\n",
    "    Ap = deepcopy(A)\n",
    "    Ap[k, i] += x\n",
    "    Ap[k, j] -= x\n",
    "    return np.linalg.norm(B - A@A, 'fro') - np.linalg.norm(B - Ap@Ap, 'fro')\n",
    "\n",
    "\n",
    "def find_root_square(B, method, decimal_point):\n",
    "    A = deepcopy(B)\n",
    "\n",
    "    matrix_size = B.shape[0]\n",
    "    old_err = 1000\n",
    "    new_err = 100\n",
    "    no_improve = 0\n",
    "\n",
    "    best_err = 1000\n",
    "    best_A = None\n",
    "\n",
    "    learning_rate = 1\n",
    "\n",
    "    iteration = 0\n",
    "\n",
    "    if(method == 'adaptive_lerning_rate'):\n",
    "        while(learning_rate >= 10**-decimal_point):\n",
    "            iteration += 1\n",
    "\n",
    "            A2 = A@A\n",
    "            E = B - A2\n",
    "\n",
    "            i, j, k = np.random.randint(0, matrix_size, 3)\n",
    "            while(i == j):\n",
    "                i, j, k = np.random.randint(0, matrix_size, 3)\n",
    "\n",
    "            if(err_improve(B, A, i, j, k, learning_rate) > 0\n",
    "                    and A[k, j] >= learning_rate):\n",
    "                A[k, i] += learning_rate\n",
    "                A[k, j] -= learning_rate\n",
    "\n",
    "            if(err_improve(B, A, i, j, k, -learning_rate) > 0\n",
    "                    and A[k, i] >= learning_rate):\n",
    "                A[k, i] += -learning_rate\n",
    "                A[k, j] -= -learning_rate\n",
    "\n",
    "            new_err = np.linalg.norm(B - A@A, 'fro')\n",
    "            # print(new_err)\n",
    "\n",
    "            if(no_improve > 100):\n",
    "                learning_rate /= 2\n",
    "                learning_rate = np.round(learning_rate, decimal_point)\n",
    "                no_improve = 0\n",
    "\n",
    "            if(best_err - new_err >= 10**-decimal_point):\n",
    "                best_err = new_err\n",
    "                no_improve = 0\n",
    "                best_A = deepcopy(A)\n",
    "            else:\n",
    "                no_improve += 1\n",
    "\n",
    "    if(method == 'closed_form'):\n",
    "        while(no_improve < 1000):\n",
    "            iteration += 1\n",
    "\n",
    "            A2 = A@A\n",
    "            E = B - A2\n",
    "\n",
    "            i, j, k = np.random.randint(0, matrix_size, 3)\n",
    "            while(i == j):\n",
    "                i, j, k = np.random.randint(0, matrix_size, 3)\n",
    "            # i, j, k = 1, 3, 1\n",
    "            C = np.zeros((matrix_size, matrix_size))\n",
    "            C[k][i], C[k][j] = 1, -1\n",
    "\n",
    "            a4, a3, a2, a1, a0 = [0]*5\n",
    "\n",
    "            sum_new_val = 0\n",
    "\n",
    "            for row in range(matrix_size):\n",
    "                for col in range(matrix_size):\n",
    "                    new_row = A[row, :] + C[row, :]\n",
    "                    new_col = A[:, col] + C[:, col]\n",
    "                    new_val = sum(new_row * new_col)\n",
    "                    sum_new_val += new_val\n",
    "                    delta_val = new_val - A2[row, col]\n",
    "                    a2 += delta_val**2\n",
    "                    a1 += 2 * delta_val * E[row, col]\n",
    "                    a0 += E[row, col]**2\n",
    "\n",
    "            roots = np.roots((a2, a1, a0))\n",
    "\n",
    "            if(roots[0] != 0):\n",
    "                root = roots[0]\n",
    "            else:\n",
    "                root = roots[1]\n",
    "            if(type(root) == np.complex_):\n",
    "                root = root.astype(float)\n",
    "            A[k][i] -= root\n",
    "            A[k][j] += root\n",
    "\n",
    "            old_err = new_err\n",
    "            new_err = np.linalg.norm(B - A@A, 'fro')\n",
    "            # if(new_err > old_err):\n",
    "            #     print(new_err)\n",
    "            if(best_err - new_err >= 10**-decimal_point):\n",
    "                best_err = new_err\n",
    "                no_improve = 0\n",
    "                best_A = deepcopy(A)\n",
    "            else:\n",
    "                no_improve += 1\n",
    "\n",
    "    return best_A, best_err, iteration\n",
    "\n",
    "\n",
    "avg_err_adaptive_lerning_rate = 0\n",
    "avg_err_closed_form = 0\n",
    "avg_iter_adaptive_lerning_rate = 0\n",
    "avg_iter_closed_form = 0\n",
    "\n",
    "for _ in range(random_test):\n",
    "    B = generate_dataset(5, decimal=decimal_point)\n",
    "    best_A, adaptive_lerning_rate_err, iteration_adaptive_lerning_rate =\\\n",
    "        find_root_square(B, 'adaptive_lerning_rate', decimal_point)\n",
    "    best_A, closed_form_err, iteration_closed_form =\\\n",
    "        find_root_square(B, 'closed_form', decimal_point)\n",
    "    avg_err_adaptive_lerning_rate += adaptive_lerning_rate_err\n",
    "    avg_iter_adaptive_lerning_rate += iteration_adaptive_lerning_rate\n",
    "    avg_err_closed_form += closed_form_err\n",
    "    avg_iter_closed_form += iteration_closed_form\n",
    "\n",
    "avg_err_adaptive_lerning_rate /= random_test\n",
    "avg_iter_adaptive_lerning_rate /= random_test\n",
    "avg_err_closed_form /= random_test\n",
    "avg_iter_closed_form /= random_test\n",
    "\n",
    "print(\n",
    "    f'The average error in {random_test}',\n",
    "    f'random matrices is by adaptive lerning rate is {avg_err_adaptive_lerning_rate/random_test}')\n",
    "print(f'The average iteration in {random_test} random matrices by adaptive lerning rate is {avg_iter_adaptive_lerning_rate/random_test}')\n",
    "print(\n",
    "    f'The average error in {random_test}',\n",
    "    f'random matrices is by closed form rate is {avg_err_closed_form/random_test}')\n",
    "print(f'The average iteration in {random_test} random matrices by closed form rate is {avg_iter_closed_form/random_test}')\n",
    "print('pause')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Based on my results I think alternate search with adaptive learning rate is the best method in my homework and it's not too slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Thank you very much for taking the time to read this</center>\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
